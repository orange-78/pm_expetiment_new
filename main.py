"""
é‡æ„åçš„ä¸»ç¨‹åº - main_refactored.py
"""

import sys
import argparse
from pathlib import Path
from typing import List, Optional
import os

import numpy as np

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from config import DataConfig, ModelConfig, TrainingConfig
from data_pipeline import DataPipeline
from model_factory import ModelFactory
from trainer import TrainingPipeline, Trainer
from model_tester import ModelTester
from data_handler import DataManager
from visualizer import plot_grid_graph, plot_pm, plot_pm_with_history
from csv_data_manager import CSVDataManager
from model_runner import ModelRunner
from config import DATA_CONFIG, MODEL_CONFIG, TRAINING_CONFIG, load_config


class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, 
                 data_config: Optional[DataConfig] = None,
                 model_config: Optional[ModelConfig] = None,
                 training_config: Optional[TrainingConfig] = None):
        
        self.data_config = data_config or DATA_CONFIG
        self.model_config = model_config or MODEL_CONFIG
        self.training_config = training_config or TRAINING_CONFIG
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data_pipeline = DataPipeline(self.data_config)
        self.model_factory = ModelFactory()
        self.trainer = Trainer(self.training_config)
        self.training_pipeline = TrainingPipeline(
            self.data_pipeline, self.model_factory, self.trainer
        )
        self.model_tester = ModelTester(self.data_config)
    
    def single_experiment(self,
                         lookback: int,
                         steps: int,
                         model_name: str,
                         model_type: str = 'lstm_attention',
                         append_params: bool = True,
                         full_batch: bool = False) -> tuple:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        
        # æ„å»ºä¿å­˜åç§°
        if append_params:
            save_name = f"{model_name}-{lookback}_{steps}"
        else:
            save_name = model_name
        
        print(f"\n{'='*60}")
        print(f"Starting experiment: {save_name}")
        print(f"Lookback: {lookback}, Steps: {steps}")
        print(f"Model type: {model_type}")
        print(f"{'='*60}")
        
        # è¿è¡Œè®­ç»ƒ
        model, history, data_info = self.training_pipeline.run_training(
            model_type=model_type,
            lookback=lookback,
            steps=steps,
            model_config=self.model_config,
            save_name=save_name,
            full_batch=full_batch
        )
        
        print(f"Experiment {save_name} completed!")
        
        return model, history, data_info
    
    def batch_experiments(self,
                         lookbacks: List[int],
                         interval: int = 30,
                         start_at: int = 0,
                         end_at: List[int] = None,
                         model_name_prefix: str = "model",
                         model_type: str = 'lstm_attention') -> List[tuple]:
        """æ‰¹é‡å®éªŒ"""
        
        if end_at is None:
            end_at = []
        
        results = []
        
        for i, lookback in enumerate(lookbacks):
            max_steps = min(end_at[i], lookback) if i < len(end_at) else lookback
            
            j = 1
            while interval * j <= max_steps:
                steps = interval * j
                
                if steps < start_at:
                    j += 1
                    continue
                
                model_name = f"{str(lookback)}_{str(steps)}/{model_name_prefix}"
                
                try:
                    result = self.single_experiment(
                        lookback=lookback,
                        steps=steps,
                        model_name=model_name,
                        model_type=model_type,
                        append_params=False
                    )
                    results.append(result)
                    
                except Exception as e:
                    print(f"Error in experiment {model_name}: {e}")
                    continue
                
                finally:
                    j += 1
        
        return results
    
    def test_model(self, model_path: str, **kwargs):
        """æµ‹è¯•å·²è®­ç»ƒçš„æ¨¡å‹"""
        return self.model_tester.run_evaluation(model_path, **kwargs)


def select_model_file(folder_paths, max_depth=3):
    """
    åœ¨å¤šä¸ªæ–‡ä»¶å¤¹åŠå…¶å­ç›®å½•ä¸­æŸ¥æ‰¾.h5æˆ–.kerasæ–‡ä»¶ï¼ŒæŒ‰å­—æ¯é¡ºåºç¼–å·å¹¶è®©ç”¨æˆ·é€‰æ‹©
    
    å‚æ•°:
    folder_paths: å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œæ–‡ä»¶å¤¹è·¯å¾„
    max_depth: int, æœ€å¤§æ£€ç´¢æ·±åº¦ï¼ˆé»˜è®¤3å±‚ï¼‰
    
    è¿”å›:
    é€‰æ‹©çš„.h5æˆ–.kerasæ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    """
    if isinstance(folder_paths, str):
        folder_paths = [folder_paths]  # å•ä¸ªè·¯å¾„è½¬ä¸ºåˆ—è¡¨
    
    all_h5_files = []
    
    for folder_path in folder_paths:
        # æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(folder_path):
            raise FileNotFoundError(f"æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder_path}")
        
        if not os.path.isdir(folder_path):
            raise NotADirectoryError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {folder_path}")

        base_depth = folder_path.rstrip(os.sep).count(os.sep)
        folder_name = os.path.basename(os.path.normpath(folder_path))

        # éå†æ–‡ä»¶å¤¹ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
        for root, dirs, files in os.walk(folder_path):
            current_depth = root.count(os.sep) - base_depth
            if current_depth >= max_depth:
                dirs[:] = []  # ä¸å†æ·±å…¥
                continue
            for file in files:
                if file.endswith(".h5") or file.endswith(".keras"):
                    full_path = os.path.join(root, file)
                    relative_path = os.path.relpath(full_path, folder_path)
                    # åœ¨ç›¸å¯¹è·¯å¾„å‰åŠ ä¸Šé¡¶å±‚ç›®å½•åï¼Œé¿å…å†²çª
                    combined_relpath = os.path.join(folder_name, relative_path)
                    all_h5_files.append((combined_relpath, full_path))

    # æŒ‰ç›¸å¯¹è·¯å¾„æ’åº
    all_h5_files.sort(key=lambda x: x[0])
    
    if not all_h5_files:
        print(f"åœ¨ {folder_paths} åŠå…¶ {max_depth} å±‚å­ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°.h5æˆ–.kerasæ–‡ä»¶")
        return None
    
    # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
    print(f"åœ¨ {folder_paths} åŠå…¶ {max_depth} å±‚å­ç›®å½•ä¸­æ‰¾åˆ° {len(all_h5_files)} ä¸ª.h5æˆ–.kerasæ–‡ä»¶:")
    print("-" * 50)
    
    for i, (rel_path, _) in enumerate(all_h5_files, 1):
        print(f"{i:2d}. {rel_path}")
    
    print("-" * 50)
    
    # è·å–ç”¨æˆ·è¾“å…¥
    while True:
        try:
            choice = input("è¯·é€‰æ‹©æ–‡ä»¶ç¼–å· (è¾“å…¥qé€€å‡º): ").strip()
            
            if choice.lower() == 'q':
                print("ç”¨æˆ·é€‰æ‹©é€€å‡º")
                return None
            
            choice_num = int(choice)
            
            if 1 <= choice_num <= len(all_h5_files):
                rel_path, full_path = all_h5_files[choice_num - 1]
                print(f"å·²é€‰æ‹©: {rel_path}")
                return full_path
            else:
                print(f"è¯·è¾“å…¥ 1-{len(all_h5_files)} ä¹‹é—´çš„æ•°å­—")
                
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
            return None



def create_custom_config():
    """åˆ›å»ºè‡ªå®šä¹‰é…ç½®çš„ç¤ºä¾‹"""
    
    # æ•°æ®é…ç½® - ä½¿ç”¨ä¸åŒçš„scalerå’Œæ®‹å·®è®¾ç½®
    data_config = DataConfig(
        model_target_dir="models_refactored",
        dataset_path="eopc04_14_IAU2000.62-now.csv",
        train_ratio=0.75,
        val_ratio=0.15,
        residual_type='both',  # 'none', 'x', 'y', 'both'
        use_scaler=True,
        scaler_type='standard',  # 'minmax', 'standard', 'robust', 'none'
        scaler_after_residual=False,  # åœ¨æ®‹å·®å¤„ç†å‰è¿˜æ˜¯ååº”ç”¨scaler
        scaler_params={'feature_range': (0, 1)} if 'minmax' else {}
    )
    
    # æ¨¡å‹é…ç½®
    model_config = ModelConfig(
        model_target_dir= "data/models_reproduce/residual-mse",
        lstm0=64,
        lstm1=64,
        lstm2=32,
        attnhead=4,  # å¢åŠ æ³¨æ„åŠ›å¤´æ•°
        attndim=32,
        dropout0=0.2,
        dropout1=0.2,
        dropout2=0.1
    )
    
    # è®­ç»ƒé…ç½®
    training_config = TrainingConfig(
        learning_rate=1e-3,
        batch_size=32,
        epochs=100,
        early_stop=10,
        loss='mae-corr',
        corr_alpha=1e-3
    )
    
    return data_config, model_config, training_config


def train_main(lookback: List[int],
               steps: int,
               model_name: str,
               use_batch: bool,
               interval: int,
               start_at: int,
               end_at: int):
    """è®­ç»ƒä¸»å‡½æ•°"""
    
    # 1. ä½¿ç”¨é»˜è®¤é…ç½®
    print("Using default configuration...")
    runner_default = ExperimentRunner()

    # # 2. ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
    # print("Using custom configuration...")
    # data_config, model_config, training_config = create_custom_config()
    # runner_custom = ExperimentRunner(data_config, model_config, training_config)
    
    if use_batch:
        # æ‰¹é‡å®éªŒ
        results = runner_default.batch_experiments(
            lookbacks=lookback,
            interval=interval,
            start_at=start_at,
            end_at=[lookback[i] - end_at for i in range(len(lookback))] if end_at <= 0
            else [end_at for i in range(len(lookback))],
            model_name_prefix=model_name
        )
    else:
        # å•ä¸ªå®éªŒ
        if len(lookback) > 1:
            print(f"only support single lookback when not batch mode, recieved {str(len(lookback))}")
        else:
            model, history, data_info = runner_default.single_experiment(
                lookback=lookback[0],
                steps=steps,
                model_name=model_name
            )
    
    print("Main function completed.")

def test_main(model_path: str = None, data_index: int = -1):
    """æµ‹è¯•ä¸»å‡½æ•°"""
    # 1. ä½¿ç”¨é»˜è®¤é…ç½®
    print("Using default configuration...")
    runner_default = ExperimentRunner()
    # 2. æµ‹è¯•æ¨¡å‹
    if not model_path:
        model_path = select_model_file(runner_default.data_config.model_target_dir, max_depth=7)
        single_test = False
    else:
        single_test = True
    while model_path:
        if os.path.exists(model_path):
            print("Testing existing model...")
            test_results = runner_default.test_model(
                model_path=model_path,
                do_predict=[0, 0, 1],  # ä»…é¢„æµ‹æµ‹è¯•é›†
                print_summary=True
            )
            T_test = np.concatenate([test_results['data']['raw_data'][4], test_results['ground_truth']['test']], axis=1)
            p_test = np.concatenate([test_results['data']['raw_data'][4], test_results['predictions']['test']], axis=1)
            plot_pm(T_test, p_test, data_index)
        else:
            print(f"{model_path} doesn't exist!")
        if single_test:
            break
        else:
            model_path = select_model_file(runner_default.data_config.model_target_dir, max_depth=7)
    
    print("Main function completed.")

def val_main(repo_path: str, model_name: str, data_path: str):
    """è¯„ä¼°ä¸»å‡½æ•°"""
    if not Path(repo_path).exists():
        raise ValueError(f"æ ¹ç›®å½•ä¸å­˜åœ¨: {repo_path}")
    
    # è·å–åŸºç¡€è¯„ä¼°ç»“æœè¡¨
    data_manager = DataManager(repo_path, excel_filename=data_path)

    # é€æ¨¡å‹è¿›è¡Œè¯„ä¼°
    config_path = f"{repo_path}/config.json"
    data_cfg, model_cfg, training_cfg = load_config(config_path)
    tester = ModelTester(data_cfg)

    for model_info in data_manager.get_existing_model_paths_with_configs(model_name):
        model_path, lookback, steps = model_info
        print(f"\n æ­£åœ¨è¯„ä¼°æ¨¡å‹: {model_path}")

        result = tester.load_and_test_model(model_path, [0, 0, 1], False)
        metrics: dict = result['metrics']['test']

        # ç¡®è®¤ç›®æ ‡è¡Œ
        rows = data_manager.locate_row_by_keys("lookback", "steps", lookback, steps)
        if not rows:
            print(f" æœªæ‰¾åˆ° lookback={lookback}, steps={steps} çš„è¡Œï¼Œè·³è¿‡ã€‚")
            continue
        row_idx = rows[0]

        # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæ•´å†™è¿‡æ•°æ®ï¼ˆå³æ‰€æœ‰æŒ‡æ ‡åˆ—éƒ½éç©ºï¼‰
        already_complete = True
        for name, content in metrics.items():
            for metric_name in content.keys():
                col_name = f"{name}_{metric_name}"
                if col_name not in data_manager.get_all_headers():
                    already_complete = False
                    break
                val = data_manager.get_row_data(row_idx)[data_manager.header_map[col_name]-1]
                if val is None:
                    already_complete = False
                    break
            if not already_complete:
                break
        if already_complete:
            print(f" è¡Œ lookback={lookback}, steps={steps} å·²å­˜åœ¨å®Œæ•´æŒ‡æ ‡ï¼Œè·³è¿‡ã€‚")
            continue

        # å†™å…¥æŒ‡æ ‡
        for name, content in metrics.items():
            for metric_name, value in content.items():
                col_name = f"{name}_{metric_name}"
                # å¦‚æœåˆ—ä¸å­˜åœ¨ â†’ æ–°å¢
                if col_name not in data_manager.get_all_headers():
                    data_manager.add_empty_column(col_name)
                # å†™å€¼
                data_manager.modify_cell_by_keys(
                    "lookback", "steps", lookback, steps, 
                    col_name, float(value), limit_one=True
                )
        print(f" å†™å…¥å®Œæˆ: lookback={lookback}, steps={steps}")

    # ä¿å­˜Excel
    data_manager.save()
    print(f"\n å·²ä¿å­˜è¯„ä¼°ç»“æœè‡³ {data_manager.get_excel_path()}")

def plot_main(repo_path: str, data_path: str):
    """ç»˜å›¾ä¸»å‡½æ•°"""
    data = DataManager(repo_path, excel_filename=data_path)
    plot_grid_graph(data.get_column_data('lookback'),
                    data.get_column_data('steps'),
                    data.get_column_data('feature_1_within_tol'),
                    title='',
                    metric_name='Within 10% Tolerance',
                    unit='',
                    scale=1.0,
                    figsize=(16, 8),
                    reverse_colorbar_num=False,
                    reverse_colorbar_color=True,
                    cmap='viridis',
                    font_size=28,
                    vrange=(0.35, 1.0))
    
def predict_main(model_path: str, csv_path: str, 
                 save_path: str = None):
    """
    ä¸»é¢„æµ‹å‡½æ•°ï¼šåŠ è½½æ¨¡å‹ã€è¯»å–CSVæœ€æ–°åºåˆ—ã€è¿›è¡Œé¢„æµ‹å¹¶å†™å…¥ç»“æœã€‚

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.keras / .h5)
        csv_path: è¦é¢„æµ‹çš„CSVæ–‡ä»¶è·¯å¾„
        train_csv_path: å¯é€‰ï¼Œç”¨äºæ‹ŸåˆScalerçš„è®­ç»ƒæ•°æ®è·¯å¾„
        save_path: å¯é€‰ï¼Œé¢„æµ‹ç»“æœä¿å­˜è·¯å¾„ï¼ˆé»˜è®¤è¦†ç›–åŸCSVï¼‰
    """

    print("=" * 60)
    print("ğŸš€ å¼€å§‹æ‰§è¡Œ predict_main()")
    print("=" * 60)

    # === 1ï¸âƒ£ åŠ è½½CSVæ•°æ® ===
    csv_manager = CSVDataManager(csv_path)
    csv_manager.print_summary()

    # === 2ï¸âƒ£ åˆ›å»ºæ¨¡å‹è¿è¡Œå™¨ ===
    # âš ï¸ æ³¨æ„ï¼šDataConfig å¿…é¡»ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼
    data_config, _, _ = load_config("config_base.json")

    runner = ModelRunner(model_path, data_config)

    # === 3ï¸âƒ£ è·å–æœ€æ–°lookbackåºåˆ— ===
    lookback = runner.lookback
    steps = runner.steps

    START_MJD = 51544
    print(f"è¯»å–ï¼ˆä¸ä¸€å®šï¼‰æœ€æ–° {lookback} æ¡è®°å½•ä½œä¸ºè¾“å…¥åºåˆ—...")
    # input_seq = csv_manager.read_latest_sequence(
    #     length=lookback
    # )
    input_seq = csv_manager.read_sequence_by_mjd(
        start_mjd=START_MJD - 1200,
        length=lookback
    )
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_seq.shape}")

    # === 4ï¸âƒ£ æ‹ŸåˆScaler ===
    runner.fit_scaler_from_data()

    # === 5ï¸âƒ£ æ¨¡å‹é¢„æµ‹ ===
    print(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹é¢„æµ‹æœªæ¥ {steps} æ­¥...")
    predictions = runner.predict(input_seq)
    print(f"âœ… é¢„æµ‹å®Œæˆ: ç»“æœå½¢çŠ¶ = {predictions.shape}")

    # === 6ï¸âƒ£ å†™å…¥é¢„æµ‹ç»“æœ ===
    print("æ­£åœ¨å°†é¢„æµ‹ç»“æœå†™å…¥CSV...")
    # csv_manager.append_predictions_from_last(predictions, save_path=save_path)
    csv_manager.write_predictions(predictions=predictions, 
                                  start_date=START_MJD)
    print("âœ… CSVå†™å…¥å®Œæˆ")

    print("=" * 60)
    print("ğŸ¯ é¢„æµ‹æµç¨‹å·²ç»“æŸ")
    print("=" * 60)

def draw_main(csv_path: str):
    """é¢„æµ‹æ•°æ®ç»˜åˆ¶å‡½æ•°"""
    # ===åŠ è½½CSVæ•°æ® ===
    csv_manager = CSVDataManager(csv_path)
    csv_manager.print_summary()

    # history_data = csv_manager.read_predictions_by_date_range('x_pole', 'y_pole',
    #                                                           '2023-7-9', '2025-9-15')
    history_data = csv_manager.read_predictions_by_date_range('x_pole', 'y_pole',
                                                              '1997-1-1', '1999-12-31')

    # bullitenA_data = csv_manager.read_predictions_by_date_range('a_x_pole_predict','a_y_pole_predict',
    #                                                             '2025-9-16', '2026-9-11')
    bullitenA_data = csv_manager.read_predictions_by_date_range('x_pole','y_pole',
                                                                '2000-1-1', '2003-1-4')
    
    # our_data = csv_manager.read_predictions_by_date_range('x_pole_predict','y_pole_predict',
    #                                                             '2025-9-16', '2027-5-8')
    our_data = csv_manager.read_predictions_by_date_range('x_pole_predict','y_pole_predict',
                                                                '2000-1-1', '2003-1-4')
    
    # plot_pm(bullitenA_data, our_data, start_date='2025-9-16')
    # plot_pm_with_history(history_data, bullitenA_data, our_data,
    #                      '2025-9-16',
    #                      legend_labels=('history', 'BulletinA', 'Our Model'))
    fig = plot_pm_with_history(history_data, bullitenA_data, our_data,
                         '2000-1-1',
                         legend_labels=('history', 'true data', 'Our Model'))
    
    
    # === è‡ªåŠ¨åˆ›å»ºç›®å½•å¹¶ä¿å­˜ ===
    save_path: str = "figures/compare.png"
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    fig.savefig(save_path, dpi=300)

    # è¿”å› Figure ä»¥ä¾¿å¤–éƒ¨ç»§ç»­æ“ä½œï¼ˆå¦‚ plt.show æˆ–å†ä¿å­˜ï¼‰

    pass


if __name__ == "__main__":
    # åˆ›å»ºè§£æå™¨
    parser = argparse.ArgumentParser(description='æ¨¡å‹è®­ç»ƒ/æµ‹è¯•è„šæœ¬')
    # æ·»åŠ å‚æ•°
    parser.add_argument('action', help='train or test')
    # æµ‹è¯•å‚æ•°
    parser.add_argument('--path', type=str, help='test model path', default=None)
    parser.add_argument('--index', type=int, help='test data index', default=-1)
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--lookback', type=int, nargs='+', help='train lookback (int or list of int)', default=[200])
    parser.add_argument('--steps', type=int, help='train steps', default=100)
    parser.add_argument('--name', type=str, help='train model name', default='model')
    parser.add_argument('--batch', action='store_true', help='train model with batch', default=False)
    parser.add_argument('--interval', type=int, help='batch steps interval', default=100)
    parser.add_argument('--startstep', type=int, help='batch steps start', default=0)
    parser.add_argument('--endstep', type=int, help='batch steps end', default=0)
    # è¯„ä¼°å’Œç»˜åˆ¶ç»“æœå›¾å‚æ•°
    parser.add_argument('--repopath', type=str, help='repo to evaluate', default='')
    parser.add_argument('--modelname', type=str, help='model name to scan', default='')
    parser.add_argument('--dataname', type=str, help='xlsx file name', default='evaluation')
    # é¢„æµ‹å‚æ•°
    parser.add_argument('--modelpath', type=str, help='prediction model path', default='')
    parser.add_argument('--csvpath', type=str, help='csv data path', default='')
    # è§£æå‚æ•°
    args = parser.parse_args()
    if args.action == "train":
        # è¿è¡Œè®­ç»ƒä¸»ç¨‹åº
        train_main(lookback=args.lookback,
                   steps=args.steps,
                   model_name=args.name,
                   use_batch=args.batch,
                   interval=args.interval,
                   start_at=args.startstep,
                   end_at=args.endstep)
    elif args.action == "test":
        # è¿è¡Œæµ‹è¯•ä¸»ç¨‹åº
        test_main(model_path=args.path,
                      data_index=args.index)
    elif args.action == "val":
        # è¿è¡Œæ¨¡å‹è¯„ä¼°ç¨‹åº
        val_main(repo_path=args.repopath,
                 model_name=args.modelname,
                 data_path=args.dataname)
    elif args.action == "plot":
        plot_main(repo_path=args.repopath,
                 data_path=args.dataname)
    elif args.action == "predict":
        predict_main(model_path=args.modelpath,
                     csv_path=args.csvpath)
    elif args.action == "draw":
        draw_main(csv_path=args.csvpath)


"""
æœ€ç»ˆæ–‡ç« ç”¨æ¨¡å‹ï¼š
mae2
mse2
mse-corr3
mse-int3
"""
